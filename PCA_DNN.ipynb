{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PCA_DNN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0uax1a1DBu56",
        "outputId": "bd7d949b-c8c3-4193-9ce9-c5aaa2e8fe2e"
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "t5QCKTG6Bxem",
        "outputId": "c81c2644-a881-493e-e5a8-6aff790a1792"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.__version__"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1.15.2'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwlncKhjB-QE"
      },
      "source": [
        "def reset_graph(seed=42):\n",
        "    tf.reset_default_graph()\n",
        "    tf.set_random_seed(seed)\n",
        "    np.random.seed(seed)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FGebCceHAk5m",
        "outputId": "d9095bc4-971d-47ad-f826-9c1ddc437098"
      },
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.contrib.layers import batch_norm, fully_connected\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "from sklearn.decomposition import PCA, IncrementalPCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "import cv2\n",
        "\n",
        "reset_graph()\n",
        "'''Preparing==============================================='''\n",
        "(X_data, y_label), (_, _) = mnist.load_data()\n",
        "n_inputs = 28*28\n",
        "n_hidden1 = 150\n",
        "n_hidden2 = 100\n",
        "n_hidden3 = 50\n",
        "n_outputs = 10\n",
        "\n",
        "X_data = np.array(X_data)\n",
        "y_label = np.array(y_label)\n",
        "\n",
        "X_data = X_data.reshape(len(X_data), 28*28).astype(np.float32)/255.0\n",
        "y_label = y_label.astype(np.int32)\n",
        "# X_test = X_test.reshape(len(X_test), 28*28).astype(np.float32)/255.0\n",
        "# y_test = y_test[:n_test].astype(np.int32)\n",
        "\n",
        "def convert_pca(X, n_batches=100, n_component=154):\n",
        "  pca = PCA()\n",
        "  inc_pca = IncrementalPCA(n_components=n_component)\n",
        "  for X_batch in np.array_split(X, n_batches):\n",
        "    print('.', end='')\n",
        "    inc_pca.partial_fit(X_batch)\n",
        "  return inc_pca.transform(X)\n",
        "\n",
        "X_data = convert_pca(X=X_data)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_data, y_label, test_size=0.33, random_state=42)\n",
        "\n",
        "m_train, n_train = X_train.shape\n",
        "\n",
        "n_epochs = 6\n",
        "batch_size = 50\n",
        "n_batches = int(np.ceil(m_train/batch_size))\n",
        "\n",
        "def fetch_batch(epoch, batch_index):\n",
        "    np.random.seed(epoch*n_batches + batch_index)\n",
        "    indices = np.random.randint(m_train, size=batch_size)\n",
        "    X_batch = X_train[indices]\n",
        "    y_batch = y_train[indices]\n",
        "    return X_train, y_train\n",
        "\n",
        "'''placeholder==============================================='''\n",
        "X = tf.placeholder(dtype=tf.float32, shape=(None, 154), name='X')\n",
        "y = tf.placeholder(dtype=tf.int32, shape=(None), name='y')\n",
        "\n",
        "is_training = tf.placeholder_with_default(False, shape=(), name='is_training')\n",
        "bn_params = {\n",
        " 'is_training': is_training,\n",
        " 'decay': 0.99,\n",
        " 'updates_collections': None\n",
        "}\n",
        "\n",
        "'''hidden layer=================================================================================================='''\n",
        "hidden1 = fully_connected(X, n_hidden1, scope=\"hidden1\", normalizer_fn=batch_norm, normalizer_params=bn_params)\n",
        "\n",
        "hidden2 = fully_connected(hidden1, n_hidden2, scope=\"hidden2\", normalizer_fn=batch_norm, normalizer_params=bn_params)\n",
        "\n",
        "hidden3 = fully_connected(hidden2, n_hidden3, scope=\"hidden3\", normalizer_fn=batch_norm, normalizer_params=bn_params)\n",
        "\n",
        "logits = fully_connected(hidden3, n_outputs, activation_fn=None,scope=\"outputs\", normalizer_fn=batch_norm, normalizer_params=bn_params)\n",
        "\n",
        "'''Loss======================================='''\n",
        "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
        "loss = tf.reduce_mean(xentropy, name='loss')\n",
        "\n",
        "'''optimizer==============================================================='''\n",
        "def learning_schedule(t):\n",
        "    return 1 / (100 * (t+1))\n",
        "\n",
        "t = tf.placeholder(dtype=tf.float32, shape=(None))\n",
        "\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_schedule(t))\n",
        "training_op = optimizer.minimize(loss)\n",
        "\n",
        "'''accuracy======================================================================================'''\n",
        "correct = tf.nn.in_top_k(logits, y, 1)\n",
        "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    for epoch in range(n_epochs):\n",
        "        for batch_index in range(n_batches):\n",
        "            X_batch, y_batch = fetch_batch(epoch, batch_index)\n",
        "            _accuracy, _loss, _ = sess.run([accuracy, loss, training_op], feed_dict={X: X_batch, y: y_batch, t: epoch})\n",
        "        _test_accuracy = sess.run(accuracy, feed_dict={X: X_test, y: y_test})\n",
        "        print(\"Epoch: {} \\ttLoss: {}, \\tTrain_Accuracy: {} \\tTest_Accuracy: {}\".format(epoch, _loss, _accuracy*100, _test_accuracy*100))\n",
        "    save_path = saver.save(sess, './my_model.ckpt')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "....................................................................................................WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/contrib/layers/python/layers/layers.py:1866: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "Epoch: 0 \ttLoss: 1.2574575521284714e-05, \tTrain_Accuracy: 100.0 \tTest_Accuracy: 97.07070589065552\n",
            "Epoch: 1 \ttLoss: 5.122397396917222e-06, \tTrain_Accuracy: 100.0 \tTest_Accuracy: 97.05050587654114\n",
            "Epoch: 2 \ttLoss: 2.9776833798678126e-06, \tTrain_Accuracy: 100.0 \tTest_Accuracy: 97.04545736312866\n",
            "Epoch: 3 \ttLoss: 1.958251004907652e-06, \tTrain_Accuracy: 100.0 \tTest_Accuracy: 97.04040288925171\n",
            "Epoch: 4 \ttLoss: 1.3616039495900623e-06, \tTrain_Accuracy: 100.0 \tTest_Accuracy: 97.04545736312866\n",
            "Epoch: 5 \ttLoss: 9.723722769194865e-07, \tTrain_Accuracy: 100.0 \tTest_Accuracy: 97.05555438995361\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8PbfE72Azh0",
        "outputId": "76e6576c-51fd-4e33-c339-df80aabe4ddd"
      },
      "source": [
        "with tf.Session() as sess:\n",
        "  saver.restore(sess, 'my_model.ckpt')\n",
        "  output = logits.eval(feed_dict={X: X_test[2].reshape(1, 154)})"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from my_model.ckpt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Lj2IDh7A3pZ",
        "outputId": "93ffbd09-b810-4fb0-acc7-53fba04cd92b"
      },
      "source": [
        "print('True: {} \\t\\t Test: {}'.format(y_test[2], np.argmax(output)))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True: 8 \t\t Test: 8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dis0Mr9pCAKl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}